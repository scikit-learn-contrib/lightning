{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# L2 solver comparison\n\nThis example compares different solvers with L2 regularization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(__doc__)\n\nimport sys\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import fetch_20newsgroups_vectorized\n\nfrom lightning.classification import SVRGClassifier\nfrom lightning.classification import SDCAClassifier\nfrom lightning.classification import CDClassifier\nfrom lightning.classification import AdaGradClassifier\nfrom lightning.classification import SAGAClassifier, SAGClassifier\n\nfrom lightning.impl.adagrad_fast import _proj_elastic_all\n\nclass Callback(object):\n\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        self.obj = []\n        self.times = []\n        self.start_time = time.time()\n        self.test_time = 0\n\n    def __call__(self, clf, t=None):\n        test_time = time.time()\n\n        if hasattr(clf, \"_finalize_coef\"):\n            clf._finalize_coef()\n\n        if t is not None:\n            _proj_elastic_all(clf.eta, t, clf.g_sum_[0], clf.g_norms_[0],\n                              alpha1=0, alpha2=clf.alpha, delta=0,\n                              w=clf.coef_[0])\n\n\n        y_pred = clf.decision_function(self.X).ravel()\n        loss = (np.maximum(1 - self.y * y_pred, 0) ** 2).mean()\n        coef = clf.coef_.ravel()\n        regul = 0.5 * clf.alpha * np.dot(coef, coef)\n        self.obj.append(loss + regul)\n        self.test_time += time.time() - test_time\n        self.times.append(time.time() -  self.start_time - self.test_time)\n\ntry:\n    dataset = sys.argv[1]\nexcept:\n    dataset = \"synthetic\"\n\nif dataset == \"news20\":\n    bunch = fetch_20newsgroups_vectorized(subset=\"all\")\n    X = bunch.data\n    y = bunch.target\n    y[y >= 1] = 1\n    alpha = 1e-4\n    eta_svrg = 1e-1\n    eta_adagrad = 1\n    xlim = (0, 20)\n\nelse:\n    X, y = make_classification(n_samples=10000,\n                               n_features=100,\n                               n_classes=2,\n                               random_state=0)\n    alpha = 1e-2\n    eta_svrg = 1e-3\n    eta_adagrad = 1e-2\n    xlim = [0, 2]\n\ny = y * 2 - 1\n\n# make sure the method does not stop prematurely, we want to see\n# the full convergence path\ntol = 1e-24\n\nclf1 = SVRGClassifier(loss=\"squared_hinge\", alpha=alpha, eta=eta_svrg,\n                      n_inner=1.0, max_iter=100, random_state=0, tol=1e-24)\nclf2 = SDCAClassifier(loss=\"squared_hinge\", alpha=alpha,\n                      max_iter=100, n_calls=X.shape[0]/2, random_state=0, tol=tol)\nclf3 = CDClassifier(loss=\"squared_hinge\", alpha=alpha, C=1.0/X.shape[0],\n                    max_iter=50, n_calls=X.shape[1]/3, random_state=0, tol=tol)\nclf4 = AdaGradClassifier(loss=\"squared_hinge\", alpha=alpha, eta=eta_adagrad,\n                    n_iter=100, n_calls=X.shape[0]/2, random_state=0)\nclf5 = SAGAClassifier(loss=\"squared_hinge\", alpha=alpha,\n                    max_iter=100, random_state=0, tol=tol)\nclf6 = SAGClassifier(loss=\"squared_hinge\", alpha=alpha,\n                    max_iter=100, random_state=0, tol=tol)\n\nplt.figure()\n\ndata = {}\nfor clf, name in ((clf1, \"SVRG\"),\n                  (clf2, \"SDCA\"),\n                  (clf3, \"PCD\"),\n                  (clf4, \"AdaGrad\"),\n                  (clf5, \"SAGA\"),\n                  (clf6, \"SAG\")\n                  ):\n    print(name)\n    cb = Callback(X, y)\n    clf.callback = cb\n\n    if name == \"PCD\" and hasattr(X, \"tocsc\"):\n        clf.fit(X.tocsc(), y)\n    else:\n        clf.fit(X, y)\n    data[name] = (cb.times, np.array(cb.obj))\n\n# get best value\nfmin = min([np.min(a[1]) for a in data.values()])\nfor name in data:\n    plt.plot(data[name][0], data[name][1] - fmin, label=name, lw=3)\n\nplt.xlim(xlim)\nplt.yscale('log')\nplt.xlabel(\"CPU time\")\nplt.ylabel(\"Objective value minus optimum\")\nplt.legend()\nplt.grid()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}